import modal
import os
import tempfile

# 1. GEREKLÄ° KÃœTÃœPHANELERÄ° TANIMLAMA (MODAL IMAGE)
# CPU MODU - cuDNN sorunlarÄ±nÄ± tamamen bypass et
whisperx_image = (
    modal.Image.debian_slim(python_version="3.11")
    .apt_install("ffmpeg", "git", "pkg-config")
    .pip_install(
        "pydub",
        "faster-whisper==1.0.3",
        "fastapi",
        "python-multipart",
        "numpy<2.0",
        "pandas",
        # LangChain + OpenAI kÃ¼tÃ¼phaneleri
        "langchain",
        "langchain-openai",
        "pydantic>=2.0",
    )
    # WhisperX'i GitHub'dan yÃ¼kle (CPU modu iÃ§in)
    .pip_install(
        "whisperx @ git+https://github.com/m-bain/whisperX.git",
    )
)

app = modal.App(name="whisperx-diarization-api", image=whisperx_image)

# Secrets
hf_secret = modal.Secret.from_name("hf-secret")
openai_secret = modal.Secret.from_name("openai-secret")

# Global deÄŸiÅŸkenler (models cache iÃ§in)
whisper_model = None
diarize_model = None
align_models_cache = {}
device = None
torch_device = None


def load_models():
    """Modelleri yÃ¼kle - CPU modunda (kararlÄ±, cuDNN sorunlarÄ± yok)"""
    global whisper_model, diarize_model, device, torch_device
    
    import whisperx
    import torch
    from pyannote.audio import Pipeline
    
    # CPU modu (cuDNN sorunlarÄ±nÄ± bypass et)
    device_str = "cpu"
    device = device_str
    torch_device = torch.device(device_str)
    
    print(f"ğŸš€ Using device: {device} (CPU mode - stable)")
    print("âš ï¸  Note: Processing will be slower but 100% stable (no cuDNN issues)")
    
    # Whisper modelini yÃ¼kle (CPU iÃ§in int8)
    print("ğŸ“¥ Loading Whisper model (large-v3)...")
    whisper_model = whisperx.load_model("large-v3", device, compute_type="int8")
    
    # Diarization modelini yÃ¼kle (CPU modunda)
    print("ğŸ“¥ Loading Diarization model (pyannote)...")
    diarize_model = Pipeline.from_pretrained(
        "pyannote/speaker-diarization-3.1",
        use_auth_token=os.environ["HF_TOKEN"]
    ).to(torch_device)
    
    print("âœ… All models loaded successfully in CPU mode!")


def get_align_model(lang_code):
    """Alignment modelini yÃ¼kle veya cache'den al"""
    import whisperx
    
    if lang_code not in align_models_cache:
        try:
            print(f"Loading alignment model for: {lang_code}")
            model, metadata = whisperx.load_align_model(language_code=lang_code, device=device)
            align_models_cache[lang_code] = (model, metadata)
        except Exception as e:
            print(f"Warning: Could not load alignment model for {lang_code}: {e}")
            return None, None
        
    return align_models_cache.get(lang_code, (None, None))


# Pydantic modelleri - Ã‡aÄŸrÄ± merkezi analizi iÃ§in
from pydantic import BaseModel, Field, field_validator
from typing import List, Optional, Union

class CallerAnalysis(BaseModel):
    """Arayan kiÅŸi analizi"""
    sentiment: str = Field(description="ArayanÄ±n genel ruh hali (Olumlu/NÃ¶tr/Olumsuz)")
    tone: str = Field(description="KonuÅŸma tonu (Sakin/Sinirli/EndiÅŸeli/Memnun)")
    main_issue: str = Field(description="ArayanÄ±n ana sorunu veya amacÄ±")
    satisfaction_level: str = Field(description="Memnuniyet seviyesi (YÃ¼ksek/Orta/DÃ¼ÅŸÃ¼k)")
    key_concerns: List[str] = Field(description="ArayanÄ±n ana endiÅŸeleri (liste)")

class AgentPerformance(BaseModel):
    """Ã‡aÄŸrÄ± merkezi personeli performans analizi"""
    professionalism_score: int = Field(description="Profesyonellik puanÄ± (0-100)", ge=0, le=100)
    empathy_score: int = Field(description="Empati puanÄ± (0-100)", ge=0, le=100)
    problem_solving_score: int = Field(description="Problem Ã§Ã¶zme puanÄ± (0-100)", ge=0, le=100)
    communication_score: int = Field(description="Ä°letiÅŸim kalitesi puanÄ± (0-100)", ge=0, le=100)
    overall_score: int = Field(description="Genel baÅŸarÄ± yÃ¼zdesi (0-100)", ge=0, le=100)
    strengths: List[str] = Field(description="GÃ¼Ã§lÃ¼ yÃ¶nler (liste)")
    improvement_areas: List[str] = Field(description="GeliÅŸim alanlarÄ± (liste)")
    key_actions: List[str] = Field(description="Personelin yaptÄ±ÄŸÄ± Ã¶nemli aksiyonlar")
    
    # Float'larÄ± int'e Ã§evir
    @field_validator('professionalism_score', 'empathy_score', 'problem_solving_score', 
                     'communication_score', 'overall_score', mode='before')
    @classmethod
    def round_scores(cls, v: Union[int, float]) -> int:
        """Float skorlarÄ± integer'a yuvarla"""
        if isinstance(v, float):
            return round(v)
        return v

class CallCenterAnalysis(BaseModel):
    """Tam Ã§aÄŸrÄ± merkezi analizi"""
    call_summary: str = Field(description="GÃ¶rÃ¼ÅŸme Ã¶zeti (2-3 cÃ¼mle)")
    call_duration_analysis: str = Field(description="GÃ¶rÃ¼ÅŸme sÃ¼resi deÄŸerlendirmesi")
    resolution_status: str = Field(description="Ã‡Ã¶zÃ¼m durumu (Ã‡Ã¶zÃ¼ldÃ¼/KÄ±smen Ã‡Ã¶zÃ¼ldÃ¼/Ã‡Ã¶zÃ¼lmedi)")
    caller_analysis: CallerAnalysis = Field(description="Arayan kiÅŸi analizi")
    agent_performance: AgentPerformance = Field(description="Personel performans analizi")
    recommendations: List[str] = Field(description="Ã–neriler (liste)")


def analyze_call_with_openai(diarized_transcript: str, language: str = "en") -> dict:
    """
    Diarization yapÄ±lmÄ±ÅŸ konuÅŸmayÄ± OpenAI ile analiz et
    
    Args:
        diarized_transcript: Speaker etiketli transkript (Ã¶rn: "SPEAKER_00: Hello\nSPEAKER_01: Hi")
        language: Tespit edilen dil
    
    Returns:
        Analiz sonuÃ§larÄ± (JSON)
    """
    from langchain_openai import ChatOpenAI
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import PydanticOutputParser
    
    print("ğŸ¤– Analyzing call with OpenAI...")
    
    # Pydantic parser oluÅŸtur
    parser = PydanticOutputParser(pydantic_object=CallCenterAnalysis)
    
    # Prompt oluÅŸtur
    system_message = """You are an expert call center quality analyst. 
Analyze the following call center conversation transcript and provide a comprehensive analysis.

The transcript shows a conversation between a customer (caller) and a call center agent.
Usually SPEAKER_00 is the agent and SPEAKER_01 is the caller, but analyze the content to determine roles.

Provide detailed analysis including:
- Call summary and resolution status
- Caller sentiment, tone, and satisfaction
- Agent performance scores (0-100) for professionalism, empathy, problem-solving, and communication
- Overall success percentage
- Strengths and areas for improvement
- Actionable recommendations

Be objective, specific, and constructive in your analysis."""

    prompt = ChatPromptTemplate.from_messages([
        ("system", system_message),
        ("human", """Analyze this call center conversation:

{transcript}

Language: {language}

{format_instructions}""")
    ])
    
    # LLM oluÅŸtur
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0.3,
        openai_api_key=os.environ["OPENAI_API_KEY"]
    )
    
    # Chain oluÅŸtur
    chain = prompt | llm | parser
    
    # Analiz yap
    try:
        result = chain.invoke({
            "transcript": diarized_transcript,
            "language": language,
            "format_instructions": parser.get_format_instructions()
        })
        
        print("âœ… OpenAI analysis completed!")
        return result.model_dump()
    
    except Exception as e:
        print(f"âŒ OpenAI analysis error: {e}")
        import traceback
        traceback.print_exc()
        raise


def process_audio_internal(audio_bytes: bytes):
    """Ses dosyasÄ±nÄ± iÅŸle"""
    import whisperx
    from pydub import AudioSegment
    import time
    
    start_time = time.time()
    print(f"ğŸµ Audio file received: {len(audio_bytes) / 1024 / 1024:.2f} MB")
    
    # GeÃ§ici dosya oluÅŸtur
    with tempfile.NamedTemporaryFile(delete=False, suffix=".tmp") as temp_in:
        temp_in.write(audio_bytes)
        temp_in_path = temp_in.name
    
    try:
        # WAV formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼r
        print("ğŸ”„ Converting to WAV format...")
        sound = AudioSegment.from_file(temp_in_path)
        sound = sound.set_frame_rate(16000).set_channels(1)
        duration = len(sound) / 1000.0  # saniye cinsinden
        print(f"â±ï¸  Audio duration: {duration:.2f}s")
        
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_out:
            sound.export(temp_out.name, format="wav")
            audio_path = temp_out.name
        
        os.remove(temp_in_path)
        
        # Ses dosyasÄ±nÄ± yÃ¼kle
        audio_input = whisperx.load_audio(audio_path)

        # Transkripsyon
        print("ğŸ¤ Transcribing audio...")
        trans_start = time.time()
        whisper_result = whisper_model.transcribe(audio_input, batch_size=16)
        detected_lang = whisper_result["language"]
        print(f"âœ… Transcription done in {time.time() - trans_start:.2f}s - Language: {detected_lang}")
        
        # Alignment
        print("ğŸ”— Aligning transcription...")
        align_start = time.time()
        align_model, align_metadata = get_align_model(detected_lang)
        
        if align_model:
            aligned_result = whisperx.align(
                whisper_result["segments"],
                align_model,
                align_metadata,
                audio_input,
                device,
                return_char_alignments=False
            )
            segments_for_diarization = aligned_result["segments"]
            print(f"âœ… Alignment done in {time.time() - align_start:.2f}s")
        else:
            segments_for_diarization = whisper_result["segments"]
            print(f"âš ï¸  No alignment model for {detected_lang}, using raw segments")
        
        # Diarization (dosya yolu gerekiyor, numpy array deÄŸil)
        print("ğŸ‘¥ Diarizing speakers...")
        diar_start = time.time()
        diarize_segments = diarize_model(audio_path)
        print(f"âœ… Diarization done in {time.time() - diar_start:.2f}s")
        
        # Speaker assignment
        print("ğŸ¯ Assigning speakers to segments...")
        assign_start = time.time()
        # Diarization annotation'Ä± pandas DataFrame'e Ã§evir
        import pandas as pd
        diarize_df = pd.DataFrame([
            {'start': turn.start, 'end': turn.end, 'speaker': speaker}
            for turn, _, speaker in diarize_segments.itertracks(yield_label=True)
        ])
        
        # assign_word_speakers(diarization_df, aligned_segments) formatÄ±
        if align_model:
            final_result = whisperx.assign_word_speakers(diarize_df, aligned_result)
        else:
            # Alignment olmadÄ±ysa whisper_result kullan
            final_result = whisperx.assign_word_speakers(diarize_df, whisper_result)
        print(f"âœ… Speaker assignment done in {time.time() - assign_start:.2f}s")
        
        # Format sonuÃ§
        diarized_text = ""
        for segment in final_result["segments"]:
            speaker = segment.get('speaker', 'UNKNOWN')
            text = segment.get('text', '').strip()
            if text:
                diarized_text += f"{speaker}: {text}\n"

        os.remove(audio_path)
        
        print(f"ğŸ‰ Transcription processing time: {time.time() - start_time:.2f}s")
        
        # OpenAI ile Ã§aÄŸrÄ± merkezi analizi yap
        analysis_start = time.time()
        try:
            call_analysis = analyze_call_with_openai(diarized_text, detected_lang)
            print(f"âœ… Analysis completed in {time.time() - analysis_start:.2f}s")
        except Exception as e:
            print(f"âš ï¸  Analysis failed: {e}, continuing without analysis")
            call_analysis = None
        
        total_time = time.time() - start_time
        print(f"ğŸ‰ Total processing time: {total_time:.2f}s")
        
        result = {
            "detected_language": detected_lang,
            "transcript_with_speakers": diarized_text,
            "raw_segments": final_result["segments"],
        }
        
        # Analiz varsa ekle
        if call_analysis:
            result["call_analysis"] = call_analysis
        
        return result
    
    except Exception as e:
        print(f"âŒ Error: {e}")
        import traceback
        traceback.print_exc()
        if os.path.exists(temp_in_path):
            os.remove(temp_in_path)
        raise


# Modal'a ASGI app'i baÄŸla (CPU modu - kararlÄ±)
@app.function(
    image=whisperx_image,
    cpu=8.0,  # 8 CPU core (hÄ±zlÄ± iÅŸlem iÃ§in)
    memory=16384,  # 16GB RAM (modeller iÃ§in)
    secrets=[hf_secret, openai_secret],  # HuggingFace + OpenAI
    timeout=900,  # CPU iÃ§in daha uzun timeout
)
@modal.asgi_app()
def fastapi_app():
    from fastapi import FastAPI, UploadFile, File
    from fastapi.responses import JSONResponse
    from fastapi.middleware.cors import CORSMiddleware
    
    web_app = FastAPI()
    
    # CORS middleware ekle (tÃ¼m origin'lere izin ver)
    web_app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    @web_app.post("/")
    async def upload_audio(audio_file: UploadFile = File(...)):
        """Ses dosyasÄ± yÃ¼kle ve iÅŸle"""
        try:
            # Modeller yÃ¼klÃ¼ deÄŸilse yÃ¼kle
            if whisper_model is None:
                load_models()
            
            # DosyayÄ± oku
            audio_bytes = await audio_file.read()
            
            # Ä°ÅŸle
            result = process_audio_internal(audio_bytes)
            
            return JSONResponse(content=result)
        
        except Exception as e:
            print(f"Endpoint error: {e}")
            return JSONResponse(
                status_code=500,
                content={"error": str(e)}
            )
    
    return web_app
